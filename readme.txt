Link to GitHub page: https://stephcpalmer.github.io/Chinese-Movie-IMDb-reviews/ 
Objective
•	In this project I aim to analyze user reviews of Chinese movies from the IMDb website and draw some inferences about the western audience's perception of Chinese cinema.
Motivation
•	This project is the final project of Professor Vierthaler's Spring 2021 CHIN 303 "Hacking Chinese Studies" class at William & Mary. From the time I was in high school, I have been beguiled by Chinese tv shows and movies. This interest in Chinese programs lead to me taking over 20 Chinese Studies credits at William & Mary even though most of them did not satisfy any of my liberal arts requirements. Thankfully, in my last semester at the College, I have found a class that intersects my academic focus in Applied Mathematics and my interest in Chinese language and culture. Hence, in the final project I wished to analyze Chinese movies, and since I am aware of IMDb's available datasets, and in my previous programming classes I have not learned how to web scrape, my ideas manifested as sourcing my analysis of Chinese movie user reviews from IMDb.
Data Gathering and Cleaning
•	I pulled data from IMDb's datasets and scraping their title and user review pages for specific movies.
IMDb Datasets
•	IMDb has subsets of their data [available](https://www.imdb.com/interfaces/) for personal and non-commercial use. Firstly, I downloaded the data from three of their datasets: title.akas, title.basics, and title.ratings using the urllib package for Python.
•	After downloading the desired zipped dataset files, I then had to unzip them. As they are gzip files, I used the python library gzip to unzip them, and I then saved the decompressed data as text files to my computer.
•	Now that I have gathered all of the data in these IMDb datasets, I must filter the data about IMDb's $10 million + titles down to only their Chinese movies data.
Cleaning IMDb Dataset Data
•	To leave only the data of movies from China, I created regular expressions using the re library to capture the title ids of titles in that have their region listed as China. This is the only data I captured from Title_akas, as it is the biggest dataset, and the other data I want to collect can be found in the other two smaller datasets. Turning to other basic data, I created a regex to capture the primary title, the original title, the start year, and genres of titles listed as movies. Finally, I created a regex to capture the average rating and the number of votes of a title.
•	Now to actually run the regular expressions and capture the data I want, I first created a dictionary to collect the Ids captured from the Id regex, and then read each line of data from Title_akas.txt to save memory and executed the search for Id's of Chinese titles.
•	Next, I searched through Title_basics.txt to capture the desired data groups from the Chinese titles captured previously:
•	1. Primary Title
•	2. Original Title
•	3. Start year
•	4. Genres
•	Starting by updating the list of Chinese movies to be only movie Ids and not all types of titles, I searched Title_ratings.txt and capture the average rating and the number of votes of Chinese movies.
•	Some of the captured Chinese movies' data is not complete, so I created a copy of the Chinese movie dictionary with the movie Id as the key and the other data as values to be able to remove the movies with incomplete data.
•	All that is left is to save the Chinese movie dictionary as Chinese_movies.txt to be able to use it for analysis. Dictionaries in python do not display well when written to a text file, so I converted the dictionary to a sting, cleaned it up, and wrote to Chinese_movies.txt with a header of my data labels and the cleaned string.
•	Upon pulling up IMDb's webpage and searching for their list of Chinese movies, the first movie that pops up on their list is 1917, which is not what I would consider to be a "Chinese" movie. IMDb includes movies that have been distributed in a certain country in their list of movies from said country, so as many movies have been distributed in China that are not movies from China, I had to verify that each movie that I collected from the datasets is actually from China.
Verifying Chinese movies
•	To verify that the data I have collected is of a movie from China, I scraped the IMDb page of each movie I collected using their title id. Before scraping from the title pages of IMDb, I checked to make sure that it is allowed in [IMDb.com/robots.txt](https://www.imdb.com/robots.txt), which it is. IMDb also has not set a crawl speed limit, so I let my urllib requests run as normal. In the IMDb pages for titles, they include the country of origin. So, I parsed through the HTML of the title pages to find the country of origin for each movie in my Chinese_movies.txt file, and if the result of the country-of-origin search showed that the movie originated from somewhere other than China, I dropped that movie from my data frame. After completing the scrape of the approximately 5,000 IMDb movie pages (which took over an hour the first time executed), my Chinese movie data frame reduced from 4,965 to 2,423, and I saved this verified Chinese movie data frame as Dataframe.txt.
•	Now that I have collected all of the data I want about the movies themselves, it is time to collect the user reviews.
Scraping User Reviews
•	User review data is not included in IMDb's available datasets--and from the size of their uncompressed datasets of much small info I can see why--so to gather the user reviews of Chinese movies, I had to scrape IMDb again. Scraping from the separate user reviews page of IMDb titles is also allowed, so I started off in an equivalent manner as when I scraped to verify the country-of-origin, except for changing the URL format. If the request to scrape from a specific movie's user reviews did not time out, which a handful did, I collected the user review itself, the user, the user rating, and the date of the review for all user reviews of that movie. Some reviews included spoilers, or did not have ratings, so mu code had to change a bit in its parsing through the HTML for the correct data to be collected.
•	After collecting the user review texts and organizing the other review and ratings data into a data frame, I saved the user reviews to User_reviews.txt and saved the data frame to User_ratings_dataframe.txt .
Analysis of User Review Data
Sentiment Analysis
What is Sentiment Analysis
•	Sentiment analysis is a natural language processing technique used in text analysis to evaluate subjective information in a source material and quantify the tone of the material. Sentiment analysis is widely used in social media monitoring to capture the public opinion and feeling held on certain topics, such as government policies or brands. Using the NLTK library, sentiment analysis is relatively simple to perform without needing an extensive background in programming.
Sentiment Analysis of User Reviews
•	The most popular natural language processing library for python is NLTK, the Natural Language Toolkit. From NLTK, the VADER lexicon must be downloaded in order to perform sentiment analysis.
•	As I have stored the user reviews in a text file and other data about the user review in a data frame, I must also import the libraries necessary to access my data. If you are performing sentiment analysis of a simple string located in the local python script, this is not necessary.
•	The texts of the user reviews in User_reviews.txt are stored in the following format: {'id': {'review # for movie': 'review text'}}. In order to form a list of just the text of the reviews, I needed to split the text file using regular expressions. First, I split the text file into each movie id and its corresponding user reviews. Then, once I was able to operate within the reviews of a single movie, if a review was detected, I added each review to the list of texts by iterating through all of the reviews for the specific movie. Since the user review was formatted in a dictionary style in the text file using pprint, it had indentations. So, I used textwrap to fill in the space of the hanging indent after the first line of each review to make the text of individual reviews a bit more legible before appending them to the list of texts.
•	Now that I have made a list of the texts of user reviews I wish to analyze, I can iterate through this list and get the sentiment analysis scores for each user review. Sentiment Intensity Analyzer or SIA, the tool performing the sentiment analysis, outputs four different scores: the positive, negative, neutral, and compound. I will only use the compound score, which is a normalized, aggregate score from the user review. If it is close to 1, the review has been analyzed to have a strong positive tone. If it close to - 1, the user review has been analyzed to have strong negative tone. If the compound score is close to 0, it does not have a strong tone either way.
•	I added all of the scores for every review into the current data frame User_ratings_df. Although I will only look at the compound score for analysis, if I return to expand on this project, I am interested in storing that information for future analysis.
•	With these sentiment analysis scores in my data frame, I compared them with the rating that each reviewer left along with their review of the movie. As I scraped the user ratings in string form, I had to evaluate the string to get a number. As the IMDb user rating system is from 1-10/10, the evaluated ratings outputted as 0 - 1. Thus, to make the compound sentiment analysis score equivalent to the user rating's range, I manipulated it using a simple math equation so that a score of 0 would be transformed to 0.5, -1 would be transformed to 0, and a score of 1 will stay 1. With the transformed user rating and compound sentiment score, I calculated the percent error of VADER's compound score. If the reviewer did not leave a rating with their review, I used the numpy library's nan constant to still represent that data as null instead of failing to append to the overall lists of errors and evaluated ratings to make placing these data points into the data frame easier.
•	Time to visualize the data! As I have approximately 8,000 user reviews, any plot I made looked too hectic, so for my visualizations I only pulled the data from a sample of the population of my data. From a past statistics class, I learned a rule-of-thumb for choosing a sample size of at least 10% the population for a representative sample, so the size I chose for my sample using pandas' Data Frame sample function is 800. I saved this sample data frame Score_sample to a text file.
•	I first plotted the counts of the different user ratings from the sample.
•	The distribution of sampled user ratings is unimodal and skewed to the left with the lowest rating count a bit higher than if it were to be perfectly skewed to the left. You can hover over the bars of the graph to the exact counts. To create this graph, I used Plotly, a python library for data visualizations. Plotly also has online functionality for creating data visualizations with [Chart Studio](https://plotly.com/chart-studio/). With a free account you can create publicly accessible visualizations, but if your data is too large it is better to use it through python as it will not work online. For example, though I created this bar chart in python, I was able to quickly recreate it using Chart Studio, but when I tried to create a visualization using my whole data frame instead of a sample on Chart Studio, it did not work.
•	Although creating this bar chart is much quicker in Chart Studio, it was not hard to create in python. First, I had to count the occurrences of each user rating level. Pandas Data Frame has a handy count function, so I called it from locating every occurrence of a certain rating level.
•	To create the bar chart, I had to import Plotly and using their built-in express, creating figures is very simple. I used the data frame formed previously as the source of the data for the figure, and by referencing the column names I was able to set my axes accordingly. I updated the x axis to show all the distinct levels of ratings instead of having the default few labelled. I also made a few other stylistic choices such as manipulating the lines and tick marks of the axes.
•	Onto visualizing the compound sentiment analysis scores!
•	They are distributed concentrated at both extremes, +1 and -1, with few in the range [-0.5,0.5]. Thus, VADER has found that most of the user reviews convey a strong tone, mostly a strong positive tone, but also a fair amount of strong negative reviews. Hovering over individual points (you can also use the tools on the top of the plot to look at the data points more closely) shows the unique review Id of the data point and the date the review was posted. The data points go from earliest on the left to most recent on the right, and in the most recent years, I would discern that the ratings are a bit more varied, especially compared to reviews between 2011-2017. This may be attributed to the western audience being more aware of international movies, and thus there are more people reviewing Chinese movies on IMDb. There does not seem to be a marked recent increase in interest towards Chinese movies specifically according to my research, but maybe plotting this data for movies of other countries may show this to be the case.
•	To create this figure, I had to transform the Date Posted data strings into datetime format, also made convenient by pandas. Creating a scatter plot of the compound scores over time was also straightforward following the Plotly [documentation](https://plotly.com/python-api-reference/generated/plotly.express.scatter).
•	Now to visualize the distribution of compound score for each rating level via box plot.
•	The compound scores for high ratings are remarkably more concentrated at the upper end of the score range, and the opposite is true for low ratings. Suitably so, the middle level rating of 0.5 has the largest range of (-0.99,0.99) and inter-quartile range (-0.74,0.96). As noted previously, the compound score is above all things a marker for the  strength of tone, so a review with rating 0.5, where the reviewer does not hold a strong opinion of the movie either way, is expected to land in the middle of the compound score range region of (-0.5,0.5). Also, it is worth noting that the median compound score for the bottom two rating levels 0.1 and 0.2 are not that different considering the bar chart which showed a large number of the lowest rating compared to the second lowest rating. I suppose that once you hold a negative opinion of a movie, it is natural to give it the worst rating possible, versus trying to find a possible merit in the movie to give it a higher score.
•	The python code to create the box plot:
Freq Dist
•	NLTK can also be used to find the most common words in a text using the FreqDist class.
•	After importing the necessary libraries, downloading NLTK modules for word tokenization and stopwords, and setting up a single string that includes all user reviews we can call to FreqDist and create a frequency distribution of all the words used in the corpus of user reviews.
•	The resulting output shows the most common words in the user reviews.
Topic Modeling
•	Topic modeling is a text analysis technique to capture the abstract topics of a corpora. To further analyze the user reviews of Chinese movies I created an LDA topic model, which trained over my corpus of user reviews. The code to my topic model can be viewed in my repository in the file Topic_model_id_reviews.py . Using the tokenized user reviews, I created a 4-topic model. The Word Clouds for each topic are below.
•	1. The first topic has common words: arts, police, hong, kong, and fight. This topic is mainly about detective films.
•	2. The second topic has common words: woman, son, father, home, and music. This topic is mainly about family.
•	3. The third topic has common words: comedy, effects, cgi, funny, kung, and fu. This topic could be concentrated on Wuxia or kung fu movies.
•	4. The fourth topic has common words: cultural, japanese, history, political, and communist. This topic is mainly about political context.
Word Embedding
•	Word embedding uses natural language processing and text analysis to analyze the meaning of a word and pull similar words from the source text.
•	Word embedding uses natural language processing and text analysis to analyze the meaning of a word and pull similar words from the source text. I used gensim's Word2Vec to look into some of the key words in the topics created from the user reviews in the word_embedding.py file.
Next Steps
Although I have completed several forms of analysis on IMDb user reviews, there is still so much more analysis left to do. To gather a more complete look, I will require fine-tuning of my topic models to improve its coherence, in order to draw some conclusions from the texts of the user reviews. Although VADER is best for informal language, it cannot differentiate when a reviewer is going over the plot versus their personal feelings towards the movie, so an improvement to the sentiment analysis lexicon could also lead to less error there.

